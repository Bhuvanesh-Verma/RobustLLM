base_model: models/Mistral-7B-v0.1 #models/Mistral-7B-Instruct-v0.2 #models/Mistral-7B-v0.1


seed: 142
patience: 100
data:
  prompt_type: train
  mode: normal
  reduce: False
  train_name: train
  val_name: dev

tokenizer:
  truncation: True
  padding: max_length
  max_length: 4096

lora_config:
  r: 32
  lora_alpha: 64
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
    - lm_head
  lora_dropout: 0.05


training:
  no_val: False
  output_dir: /netscratch/bverma/results
  run_name: inference
  max_steps: 1000
  num_train_epochs: 3
  batch_size: 2
  #patience: 7 # means 7 evaluation cycles
  optim:
    learning_rate: 0.00005
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.999
    epsilon: 0.00000008

aux:
  hidden_size: 64
  lr: 0.0001


logger:
  name: wandb
  project: acc_llm
  entity: bhuvi
  dir: /netscratch/bverma/logs
  mode: offline
